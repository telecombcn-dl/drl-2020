{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"arap_2020_lab02_reinforce_todo.ipynb","provenance":[{"file_id":"1WmsBPmUQ-6BPhKd_NoQlA5IcJdFTWFjF","timestamp":1612031513505},{"file_id":"1kf4xXPIXVigFd8ENaL1MStv5q0TUYHLo","timestamp":1612031280909}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0spTu6Dawi2S"},"source":["# Basic Policy Gradients (REINFORCE) example in PyTorch\n","\n","This notebook is adapted from the [official REINFORCE tutorial](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)."]},{"cell_type":"markdown","metadata":{"id":"hIYC7IeEwrlt"},"source":["## Installing dependencies\n","\n","We will use OpenAI Gym to simulate the environment, which might not be installed by default. We also need to install some dependencies for visualization purposes (this may take a while)."]},{"cell_type":"code","metadata":{"id":"Pm4IZndds2u2"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzOcEmlJw6gb"},"source":["## Setting up the environment\n","\n","We will need some tricks to visualize the simulations in the browser, as simply calling env.render() will not work in this notebook ([source](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=Jyb2Ujuozfi2&forceEdit=true&offline=true&sandboxMode=true))."]},{"cell_type":"code","metadata":{"id":"cElJ0d4PsX5X"},"source":["import gym\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import namedtuple"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHiHbaL85e1k"},"source":["from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DmTRRyhmgYt"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnf0NPZ_ss8u"},"source":["print('PyTorch version: ', torch.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUlPxFV1xtw4"},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_b9tRuVxwP3"},"source":["def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBKofZ4KrG8Z"},"source":["## Visualize a random policy in the environment\n","\n","Our goal is to train an agent that is capable of solving the CartPole problem, where a pole is attached to a cart moving along a horizontal track. The agent can interact with the environment by applying a force (+1/-1) to the cart. The episode is terminated whenever the pole is more than 15 degrees from vertical or the cart goes out of bounds in the horizontal axis. The agent receives +1 reward for each timestep under the desired conditions.\n","\n","We can visualize what a random policy would do in this environment:"]},{"cell_type":"code","metadata":{"id":"mH913G8v01dn"},"source":["# Let's generate a random trajectory...\n","env = wrap_env(gym.make(\"CartPole-v1\"))\n","ob, done, total_rew = env.reset(), False, 0\n","while not done:\n","  env.render()\n","  ac = env.action_space.sample()\n","  ob, rew, done, info = env.step(ac)\n","  total_rew += rew\n","print('Cumulative reward:', total_rew)\n","  \n","# ... and visualize it!\n","env.close()\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeZXG6VHygrD"},"source":["## Create the model\n","\n","Now we will define our policy, parameterized by a feedforward neural network.\n","\n","**Exercise #1.** Implement the policy as an MLP with a hidden layer of 128 neurons with a ReLU activation and a Softmax output layer."]},{"cell_type":"code","metadata":{"id":"jKr_lu19yc3B"},"source":["class Policy(nn.Module):\n","    def __init__(self, inputs, outputs):\n","        super(Policy, self).__init__()\n","        # TODO\n","        self.affine1 = nn.TODO\n","        self.affine2 = nn.TODO\n","\n","        self.saved_log_probs = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        # TODO\n","        x = self.TODO\n","        x = F.TODO\n","        action_scores = self.TODO\n","        return F.softmax(action_scores, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0T02F2uyndg5"},"source":["## Functions for collecting experience and updating the policy\n","\n","**Exercise #2.** Use the policy to predict a distribution of probabilities across actions.\n","\n","**Exercise #3.** Compute the return from the rewards collected by the policy.\n","\n","**Exercise #4.** Complete the loss computation using the returns and the log probs."]},{"cell_type":"code","metadata":{"id":"lcXYBZNznd96"},"source":["def select_action(policy, state):\n","    # Convert state into PyTorch tensor\n","    state = torch.from_numpy(state).float().unsqueeze(0)\n","    # TODO: Compute action probabilities\n","    probs = TODO\n","    # Sample action\n","    m = torch.distributions.Categorical(probs)\n","    action = m.sample()\n","    # Bookkeeping\n","    policy.saved_log_probs.append(m.log_prob(action))\n","    return action.item()\n","\n","\n","def train(policy, optimizer):\n","    R = 0\n","    policy_loss = []\n","    returns = []\n","    # Compute the returns by reading the rewards vector backwards\n","    for r in policy.rewards[::-1]:\n","        # TODO: Complete the computation of the return using gamma\n","        R = r + TODO\n","        returns.insert(0, R)\n","    returns = torch.tensor(returns)\n","    # Normalize returns (this usually accelerates convergence)\n","    returns = (returns - returns.mean()) / (returns.std() + eps)\n","    for log_prob, R in zip(policy.saved_log_probs, returns):\n","        # TODO: Complete the 'loss' computation using the returns and the log probs.\n","        policy_loss.append(TODO)\n","    # Update policy: \n","    #  (1) reset optimizer grads\n","    optimizer.zero_grad()\n","    #  (2) compute surrogate policy gradients loss\n","    policy_loss = torch.cat(policy_loss).sum()\n","    #  (3) SGD step\n","    policy_loss.backward()\n","    optimizer.step()\n","    del policy.rewards[:]\n","    del policy.saved_log_probs[:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr4pSznznosA"},"source":["## Training the agent"]},{"cell_type":"code","metadata":{"id":"0tJ7RcgfnxPa"},"source":["# Hyperparameters\n","env_name = 'CartPole-v1'\n","gamma = 0.99  # discount factor\n","seed = 543  # random seed\n","log_interval = 10  # controls how often we log progress\n","max_ep_len = 1000  # maximum episode length\n","num_episodes = 1500  # number of episodes to train on"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLb1oPuGoJpJ"},"source":["# Create environment\n","env = gym.make(env_name)\n","\n","# Fix random seed (for reproducibility)\n","env.seed(seed)\n","torch.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AGdVUq4nrWg"},"source":["# Create policy and optimizer\n","n_inputs = env.observation_space.shape[0]\n","n_actions = env.action_space.n\n","policy = Policy(n_inputs, n_actions)\n","optimizer = torch.optim.Adam(policy.parameters(), lr=1e-2)\n","eps = np.finfo(np.float32).eps.item()\n","\n","\n","# Training loop\n","print(\"Target reward: {}\".format(env.spec.reward_threshold))\n","running_reward = 10\n","ep_rew_history_reinforce = []\n","for i_episode in range(num_episodes):\n","    # Collect experience\n","    state, ep_reward = env.reset(), 0\n","    for t in range(max_ep_len):  # Don't infinite loop while learning\n","        \n","        action = select_action(policy, state)\n","        state, reward, done, _ = env.step(action)\n","        policy.rewards.append(reward)\n","        ep_reward += reward\n","        if done:\n","            break\n","\n","    # Update running reward\n","    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n","    \n","    # Perform training step\n","    train(policy, optimizer)\n","    ep_rew_history_reinforce.append((i_episode, ep_reward))\n","    if i_episode % log_interval == 0:\n","        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n","              i_episode, ep_reward, running_reward))\n","    if running_reward > env.spec.reward_threshold:\n","        print(\"Solved!\")\n","        break\n","\n","print(\"Finished training! Running reward is now {:.2f} and \"\n","      \"the last episode runs to {} time steps!\".format(running_reward, t))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mv90goqpVguY"},"source":["plt.plot([x[0] for x in ep_rew_history_reinforce], [x[1] for x in ep_rew_history_reinforce])\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cESm8xBnFj-"},"source":["## Visualize trained policy"]},{"cell_type":"code","metadata":{"id":"F97G8ZBUyPAN"},"source":["test_env = wrap_env(gym.make(env_name))\n","state, ep_reward, done = test_env.reset(), 0, False\n","while not done:\n","    test_env.render()\n","    action = select_action(policy, state)\n","    state, reward, done, _ = test_env.step(action)\n","    ep_reward += reward\n","print(\"Cumulative reward: {}\".format(ep_reward))\n","\n","test_env.close()\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u9TcQBXC03u6"},"source":["# REINFORCE w/ Baseline Version"]},{"cell_type":"markdown","metadata":{"id":"RUqp44g904Gm"},"source":["## The new Policy Module encodes both Actor and Critic's Network.\n","**Exercise #5.** Complete the forward pass using the corresponding actor and critic's heads.\n"]},{"cell_type":"code","metadata":{"id":"IEgGgA-m04PN"},"source":["class Policy(nn.Module):\n","    \"\"\"\n","    implements both actor and critic in one model\n","    \"\"\"\n","    def __init__(self, inputs, actor_output, critic_output):\n","        super(Policy, self).__init__()\n","        self.affine1 = nn.Linear(inputs, 128)\n","\n","        # actor's layer\n","        self.actor_head = nn.Linear(128, actor_output)\n","\n","        # critic's layer\n","        self.critic_head = nn.Linear(128, critic_output)\n","\n","        # action & reward buffer\n","        self.saved_log_probs = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward of both actor and critic\n","        \"\"\"\n","        x = F.relu(self.affine1(x))\n","\n","        # returns probability of each action\n","        action_prob = F.softmax(TODO, dim=-1)\n","\n","        # evaluates being in the state s_t\n","        state_values = TODO\n","\n","        # return values for both actor and critic as a tuple of 2 values:\n","        # 1. a list with the probability of each action over the action space\n","        # 2. the value from state s_t \n","        return action_prob, state_values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VdU5IOgu189C"},"source":["## Now we want to store also the state-value for each step"]},{"cell_type":"code","metadata":{"id":"HwRXvjHO04lI"},"source":["def select_action(model, state):\n","    # Convert state into PyTorch tensor\n","    state = torch.from_numpy(state).float().unsqueeze(0)\n","    # Compute action probabilities and state value\n","    probs, state_value = model(state)\n","    # Sample action\n","    m = torch.distributions.Categorical(probs)\n","    action = m.sample()\n","    # Bookkeeping\n","    model.saved_log_probs.append(SavedAction(m.log_prob(action), state_value))\n","\n","    return action.item()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUKEn_-P1_gM"},"source":["## The gradient computation\n","$$ g = \\mathbb{E}\\left[{\\sum_{t=0}^{\\infty} \\Psi_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t)}\\right]$$\n","## with REINFORCE, $\\Psi_t $ was defined as \n","$$ \\Psi_t =  \\sum_{t'=t}^\\infty r_{t'} $$\n","## but now we will substract a baseline learned with the Critic Network\n","$$ \\Psi_t =  \\sum_{t'=t}^\\infty (r_{t'}-b(s_t)) $$\n","\n","**Exercise #6.** Compute the advantages by substracting the baselines to the returns."]},{"cell_type":"code","metadata":{"id":"KqonQaAd1_6l"},"source":["def train(model, optimizer):\n","    R = 0\n","    policy_losses = []\n","    value_losses = []\n","    returns = []\n","    # Compute discounted rewards\n","    for r in model.rewards[::-1]:\n","        R = r + TODO\n","        returns.insert(0, R)\n","    returns = torch.tensor(returns)\n","    # Normalize returns (this usually accelerates convergence)\n","    returns = (returns - returns.mean()) / (returns.std() + eps)\n","\n","    for (log_prob, baseline), R in zip(model.saved_log_probs, returns):\n","        advantage = TODO\n","\n","        # calculate actor (policy) loss \n","        policy_losses.append(-log_prob * advantage)\n","\n","        # calculate critic (value) loss using L1 smooth loss\n","        value_losses.append(F.smooth_l1_loss(baseline, torch.tensor([R])))\n","\n","\n","    # Update policy: \n","    #  (1) reset optimizer grads\n","    optimizer.zero_grad()\n","    #  (2) compute surrogate policy gradients loss\n","    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n","    #  (3) SGD step\n","    loss.backward()\n","    optimizer.step()\n","    del model.rewards[:]\n","    del model.saved_log_probs[:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_alUTcqe2bLN"},"source":["## Train the agent again"]},{"cell_type":"code","metadata":{"id":"5qgA5laR2AC1"},"source":["# Create environment\n","SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n","env = gym.make(env_name)\n","\n","# Fix random seed (for reproducibility)\n","env.seed(seed)\n","torch.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sRSy2ZSZ2ei5"},"source":["# Create policy and optimizer\n","n_inputs = env.observation_space.shape[0]\n","n_actions = env.action_space.n\n","policy = Policy(n_inputs, n_actions, 1)\n","optimizer = torch.optim.Adam(policy.parameters(), lr=1e-2)\n","eps = np.finfo(np.float32).eps.item()\n","\n","\n","# Training loop\n","print(\"Target reward: {}\".format(env.spec.reward_threshold))\n","running_reward = 10\n","ep_rew_history_baseline = []\n","for i_episode in range(num_episodes):\n","    # Collect experience\n","    state, ep_reward = env.reset(), 0\n","    for t in range(max_ep_len):  # Don't infinite loop while learning\n","        action = select_action(policy, state)\n","        state, reward, done, _ = env.step(action)\n","        policy.rewards.append(reward)\n","        ep_reward += reward\n","        if done:\n","            break\n","\n","    # Update running reward\n","    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n","    \n","    # Perform training step\n","    train(policy, optimizer)\n","    ep_rew_history_baseline.append((i_episode, ep_reward))\n","    if i_episode % log_interval == 0:\n","        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n","              i_episode, ep_reward, running_reward))\n","    if running_reward > env.spec.reward_threshold:\n","        print(\"Solved!\")\n","        break\n","\n","print(\"Finished training! Running reward is now {:.2f} and \"\n","      \"the last episode runs to {} time steps!\".format(running_reward, t))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuIWylVK2jCx"},"source":["fig, ax = plt.subplots(figsize=(16,10))\n","plt.plot([x[0] for x in ep_rew_history_reinforce], [x[1] for x in ep_rew_history_reinforce])\n","plt.plot([x[0] for x in ep_rew_history_baseline], [x[1] for x in ep_rew_history_baseline])\n","plt.legend(['REINFORCE', 'REINFORCE w/ baseline'])\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3trZ4eK2mHA"},"source":["## Visualize trained policy"]},{"cell_type":"code","metadata":{"id":"XdRfVvQS2jvD"},"source":["test_env = wrap_env(gym.make(env_name))\n","state, ep_reward, done = test_env.reset(), 0, False\n","while not done:\n","    test_env.render()\n","    action = select_action(policy, state)\n","    state, reward, done, _ = test_env.step(action)\n","    ep_reward += reward\n","print(\"Cumulative reward: {}\".format(ep_reward))\n","\n","test_env.close()\n","show_video()"],"execution_count":null,"outputs":[]}]}