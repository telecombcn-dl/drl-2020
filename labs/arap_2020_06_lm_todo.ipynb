{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dlai_2020_10_lm_todo.ipynb","provenance":[{"file_id":"16FQ1kgG8WmunbmzDBjkAzkpgEaiWyUo0","timestamp":1607534236342},{"file_id":"102xl1ob9cv7gbI3SApZNtHUMoqzRYaeb","timestamp":1602535118176},{"file_id":"15kA4S2H_ai3eUJWM34G-1nj-inbFSRqR","timestamp":1602533131388},{"file_id":"1-9KJsxcbi_WeOinq86RNazVyEtX8Vm4S","timestamp":1593173101288},{"file_id":"1z1_-QEBHSotQKsFn2AVyg0EnFoTAyyZo","timestamp":1593173053348},{"file_id":"1cbFYHWM2v2jcKSjBLd9prMzkKY1AN53w","timestamp":1581978400793}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2DZwK1xpCd9a"},"source":["# Language Models\n","\n","**Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).**\n"]},{"cell_type":"code","metadata":{"id":"4OdVfmDx8LoQ"},"source":["import torch\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","import time\n","import torchtext\n","from torchtext.data.utils import get_tokenizer\n","if not torch.cuda.is_available():\n","    raise RuntimeError(\"You should enable GPU runtime.\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OjoZl4ksHQcN"},"source":["A language model is a probability distribution over sequences of words. To train a Deep Learning language model, we will task the model to, given a sequence of words, predict the following one.\n","\n","For this lab, we will train 2 different models. The first one will be a simple RNN model with a encoder decoder structure. The second one will be a Transformer Model."]},{"cell_type":"markdown","metadata":{"id":"AnE3PTxOKxHI"},"source":["# RNN Model"]},{"cell_type":"markdown","metadata":{"id":"OZToZpN8aBoO"},"source":["First we will declare the model that we will use. We will start with a simple RNN model made of an encoder, a recurrent module, and a decoder."]},{"cell_type":"markdown","metadata":{"id":"QYw3yv_RlO1d"},"source":["### Model"]},{"cell_type":"markdown","metadata":{"id":"KJ9EImaubusi"},"source":["##### **Exercise 1**\n","Write the forward method of the model, using the encoder layer, the rnn, and the decoder. Note that the forward method should return both the decoded output and the hidden state from the RNN. Use the dropout layer after the encoder layer."]},{"cell_type":"code","metadata":{"id":"ktBXAZz98N8Y"},"source":["class RNNModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n","\n","    def __init__(self, ntoken, embedding_size, nhid, nlayers, dropout=0.5, pretrained_embeddings=None):\n","        super().__init__()\n","\n","        self.pretrained_embeddings = pretrained_embeddings\n","        if pretrained_embeddings is None:\n","            self.encoder = nn.Embedding(ntoken, embedding_size)\n","        self.drop = nn.Dropout(dropout)\n","        self.rnn = nn.LSTM(embedding_size, nhid, nlayers, dropout=dropout)\n","        self.decoder = nn.Linear(nhid, ntoken)\n","\n","        self.nhid = nhid\n","        self.nlayers = nlayers\n","\n","    def forward(self, x, hidden):\n","        ...\n","        return decoded, hidden\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters())\n","        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n","                weight.new_zeros(self.nlayers, bsz, self.nhid))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xk83j8QUbfGK"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"8oo-Q2k4FoKq"},"source":["batch_size = 20\n","bptt = 35  # Back Propagation Through Time\n","embedding_size = 300  # 650 gives better results, but is much slower\n","hidden_size = 300  # 650 gives better results, but is much slower\n","n_layers = 2\n","lr = 1e-2\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NLpK2vp9blMb"},"source":["### Data loading\n","\n","For our task we will use the WikiText2 dataset. This language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n","\n","Starting from sequential data. We will arrange the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n","```\n","┌ a g m s ┐\n","│ b h n t │\n","│ c i o u │\n","│ d j p v │\n","│ e k q w │\n","└ f l r x ┘\n","```\n","These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient batch processing."]},{"cell_type":"code","metadata":{"id":"pCsB-ZOgFaA7"},"source":["dataset = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n","                            init_token='<sos>',\n","                            eos_token='<eos>',\n","                            lower=True)\n","train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(dataset)\n","\n","# build the vocabulary\n","dataset.build_vocab(train_txt)\n","ntokens = len(dataset.vocab.stoi)  # stoi = string to int\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vr2J0VHQu9O7"},"source":["# make iterator for splits\n","train_iter, valid_iter, test_iter = torchtext.data.BPTTIterator.splits(\n","    (train_txt, val_txt, test_txt), batch_size=batch_size, bptt_len=bptt, device=device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zx9MNZgobret"},"source":["### Instantiate model\n","\n","Here, we instantiate the model and the optimizer. We will also use a LR scheduler to decrease the LR after every epoch."]},{"cell_type":"markdown","metadata":{"id":"h-JhIiMoid8D"},"source":["#### **Exercise 2**\n","\n","Instantiate the model with the correct hyperparameters. Then, instantiate also the correct loss function for a language model, the Adam optimizer and a [StepLR](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR) learning rate scheduler with step 1 and gamma 0.95."]},{"cell_type":"code","metadata":{"id":"n5uJkYP3G2fp"},"source":["model = ...\n","\n","criterion = ...\n","optimizer = ...\n","scheduler = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ixURcUlVb6PP"},"source":["### Train function\n","\n","Now we define the train function that trains the model for an epoch."]},{"cell_type":"markdown","metadata":{"id":"kILemaj8gSbI"},"source":["#### **Exercise 3**\n","\n","Complete the training function with help of the code comments. You can check the documentation of [clip_grad_norm_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_). "]},{"cell_type":"code","metadata":{"id":"bOJhvX3zcAwQ"},"source":["def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)  # For LSTMs\n","\n","\n","def train():\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    hidden = model.init_hidden(batch_size)\n","    for i, batch in enumerate(train_iter):\n","        data, target = batch.text, batch.target\n","\n","        # Set gradients to zero\n","        ...\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        hidden = repackage_hidden(hidden)\n","\n","        # Compute the output and the new hidden state\n","        output, hidden = ...\n","\n","        output = output.permute(0, 2, 1)\n","        loss = criterion(output, target)\n","        loss.backward()\n","\n","        # use `clip_grad_norm_` to clip the norm of the gradients to 0.25. It will help the training of the rnn\n","        ...\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        log_interval = 100\n","        if i % log_interval == 0 and i > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print(f'| epoch {epoch:3d} | {i:5d}/{len(train_iter):5d} batches | lr {lr:.4f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n","            total_loss = 0\n","            start_time = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zvq79Xn9b8xt"},"source":["### Validation function\n","\n","Now we will define the validation function, that given a dataset (val or test) will evaluate the loss of the prediction of the model in that dataset."]},{"cell_type":"markdown","metadata":{"id":"Xlg_cNacb3Zy"},"source":["#### **Exercise 4**\n","\n","Complete the validation function to compute the loss. `data_source` corresponds to `val_data` or `test_data` (depending on which phase of the training code we are).\n","\n"]},{"cell_type":"code","metadata":{"id":"c_Bnq5RPcCEl"},"source":["@torch.no_grad()\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0.\n","    n = 0\n","    hidden = ...\n","    for i, batch in enumerate(data_source):\n","        data, target = batch.text, batch.target\n","        output, hidden = ....\n","        output = output.permute(0, 2, 1)\n","        total_loss += target.numel() * criterion(output, target).item()\n","        n += target.numel()\n","    return total_loss / n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQxGZc2TcEkO"},"source":["### Training loop\n","\n","This is the training loop code. At any point you can hit stop to get out of training early."]},{"cell_type":"code","metadata":{"id":"xI88vCE7_E5J"},"source":["best_val_loss = float(\"inf\")\n","\n","# At any point you can hit stop to get out of training early.\n","epochs = 4\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(valid_iter)\n","        print('-' * 89)\n","        print(f'| end of epoch {epoch} | time: {(time.time() - epoch_start_time):.2f}s | valid loss {val_loss:.2f} | valid ppl {math.exp(val_loss):.2f}')\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if val_loss < best_val_loss:\n","            with open(\"best_checkpoint.pth\", 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        scheduler.step()\n","        lr = scheduler.get_last_lr()[0]\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(\"best_checkpoint.pth\", 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    model.rnn.flatten_parameters()\n","\n","# Run on test data. \n","with torch.no_grad():\n","    test_loss = evaluate(test_iter)\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n","print('=' * 89)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EamPZGYGcd1_"},"source":["### Text generation"]},{"cell_type":"markdown","metadata":{"id":"ghEye4mBJmCZ"},"source":["Now we can test the performance of our language model, by first inputting a random word to the model, generating a new word (by taking the most likely output from the model) and then inputting the generated word to the model iteratively."]},{"cell_type":"code","metadata":{"id":"fzKMoYJT_rJP"},"source":["model.eval()\n","\n","hidden = model.init_hidden(bsz=1)\n","x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","temperature = 1  # Higher will increase diversity\n","text = \"\"\n","with torch.no_grad():\n","    for i in range(1000):\n","        output, hidden = model(x, hidden)\n","        word_weights = (output / temperature).exp().cpu().squeeze()  # Softmax without normalizing\n","        word_idx = torch.multinomial(word_weights, 1)[0]\n","        x = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n","        word = dataset.vocab.itos[word_idx]\n","        text += word + ('\\n' if i % 20 == 19  or word == '<eos>' else ' ')\n","\n","print(text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppSA30_FyJeS"},"source":["# Use pretrained embeddings"]},{"cell_type":"code","metadata":{"id":"RA3jdPbRxCXr"},"source":["dataset = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),\n","                            init_token='<sos>',\n","                            eos_token='<eos>',\n","                            lower=True)\n","train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(dataset)\n","\n","# build the vocabulary\n","embeddings = torchtext.vocab.GloVe(name='6B', dim=300)\n","dataset.build_vocab(train_txt, vectors=embeddings)  # Specify the embedding https://nlp.stanford.edu/projects/glove/\n","ntokens = len(dataset.vocab.stoi)  # stoi = string to int\n","print(dataset.vocab.vectors.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7DXC43XxkxW"},"source":["def get_embedding(word):\n","    index = dataset.vocab.stoi[word]\n","    return dataset.vocab.vectors[index]\n","\n","print(get_embedding(\"queen\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVUkUaaDhEV5"},"source":["#### **Exercise 5**\n","\n","Complete the code to find the closest embeddings to `queen` - `woman` + `man`. You can use `torch.dist` to compute the distance between 2 vectors.\n"]},{"cell_type":"code","metadata":{"id":"gc-xkHiW1Qhu"},"source":["embedding = ... - ... + ...\n","\n","distances = []\n","for i, vector in enumerate(dataset.vocab.vectors):\n","    if dataset.vocab.stoi[\"queen\"] != i:\n","        distance = ...\n","        distances.append(distance)\n","    else:\n","        distances.append(torch.tensor(float(\"inf\")))\n","distances = torch.stack(distances)\n","indices = torch.topk(-distances, k=5)[1]\n","print([dataset.vocab.itos[ind] for ind in indices])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0FAZkVYhZhq"},"source":["#### **Exercise 6**\n","\n","Feel free to try on your own to see what embeddings are close to each other"]},{"cell_type":"code","metadata":{"id":"s6pbsRgH3-ox"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZNy_qcM6abP"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"A9zTHr4y6ZfY"},"source":["model = RNNModel(ntokens, embedding_size, hidden_size, n_layers, pretrained_embeddings=dataset.vocab.vectors.to(device)).to(device)\n","lr = 1e-3\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X1JBh42R7c4e"},"source":["## Training loop"]},{"cell_type":"code","metadata":{"id":"fucVUohz7ceU"},"source":["best_val_loss = float(\"inf\")\n","\n","# At any point you can hit stop to get out of training early.\n","epochs = 4\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(valid_iter)\n","        print('-' * 89)\n","        print(f'| end of epoch {epoch} | time: {(time.time() - epoch_start_time):.2f}s | valid loss {val_loss:.2f} | valid ppl {math.exp(val_loss):.2f}')\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if val_loss < best_val_loss:\n","            with open(\"best_checkpoint.pth\", 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        scheduler.step()\n","        lr = scheduler.get_last_lr()[0]\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(\"best_checkpoint.pth\", 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    model.rnn.flatten_parameters()\n","\n","# Run on test data. \n","with torch.no_grad():\n","    test_loss = evaluate(test_iter)\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n","print('=' * 89)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SK4NABKtdfj7"},"source":["### Text generation"]},{"cell_type":"code","metadata":{"id":"uG9bZ4CCdhDz"},"source":["model.eval()\n","\n","hidden = model.init_hidden(bsz=1)\n","x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","temperature = 1  # Higher will increase diversity\n","text = \"\"\n","with torch.no_grad():\n","    for i in range(1000):\n","        output, hidden = model(x, hidden)\n","        word_weights = (output / temperature).exp().cpu().squeeze()  # Softmax without normalizing\n","        word_idx = torch.multinomial(word_weights, 1)[0]\n","        x = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n","        word = dataset.vocab.itos[word_idx]\n","        text += word + ('\\n' if i % 20 == 19  or word == '<eos>' else ' ')\n","\n","print(text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j0NQrOFgK1gd"},"source":["# Extra: Transformer Model"]},{"cell_type":"markdown","metadata":{"id":"P1ppcvdgRiLX"},"source":["Now we will train a Transformer to solve the language modelling task. The structure of the architecture of the model is the following:\n","\n","![alt text](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)"]},{"cell_type":"markdown","metadata":{"id":"pnbNdWveLJIP"},"source":["Even though it seems complicated, with PyTorch and the `nn.TransformerEncoder` module this can be implemented in an easy (or at least, easier) way."]},{"cell_type":"markdown","metadata":{"id":"6dJgJSNtcoaV"},"source":["### Positional Encoder"]},{"cell_type":"markdown","metadata":{"id":"kqcu6ov_M6iw"},"source":["First, we will use a Positional Encoding module. Positional Encoding injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use sine and cosine functions of different frequencies."]},{"cell_type":"code","metadata":{"id":"xVbG3Va5M1ug"},"source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        r\"\"\"Inputs of forward function\n","        Args:\n","            x: the sequence fed to the positional encoder model (required).\n","        Shape:\n","            x: [sequence length, batch size, embed dim]\n","            output: [sequence length, batch size, embed dim]\n","        Examples:\n","            >>> output = pos_encoder(x)\n","        \"\"\"\n","\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awFXvAySlxnq"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"b-kI_roTKgix"},"source":["from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","\n","class TransformerModel(nn.Module):\n","    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super().__init__()\n","        self.ninp = ninp\n","        self.src_mask = None\n","\n","        self.encoder = nn.Embedding(ntoken, ninp)\n","\n","        self.pos_encoder = PositionalEncoding(ninp, dropout)\n","\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n","        \n","        self.decoder = nn.Linear(ninp, ntoken)\n","\n","    def _generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)  # Lower triangular matrix with ones.\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def forward(self, x, has_mask=True):\n","        if has_mask:\n","            device = x.device\n","            if self.src_mask is None or self.src_mask.size(0) != len(x):\n","                mask = self._generate_square_subsequent_mask(len(x)).to(device)\n","                self.src_mask = mask\n","        else:\n","            self.src_mask = None\n","\n","        embeddings = self.encoder(x)\n","        embeddings = embeddings * math.sqrt(self.ninp)\n","        embeddings = self.pos_encoder(embeddings)\n","\n","        encoded = self.transformer_encoder(embeddings, self.src_mask)\n","        decoded = self.decoder(encoded)\n","        return decoded\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8CT8eIecvKF"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"sbAXoqQCOIKq"},"source":["batch_size = 20\n","bptt = 35  # Back Propagation Through Time\n","embedding_size = 300  # 650 gives better results, but is much slower\n","hidden_size = 300  # 650 gives better results, but is much slower\n","n_layers = 2\n","n_heads = 2  # Transformer heads\n","lr = 1e-3\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PYwbgmMbdGJ9"},"source":["### Model\n","Here, we instantiate the model and the optimizer. We will also use a LR scheduler to decrease the LR after every epoch."]},{"cell_type":"code","metadata":{"id":"vBoxXR8ADucK"},"source":["model = TransformerModel(ntokens, embedding_size, n_heads, hidden_size, n_layers).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59HqXgXudJXq"},"source":["### Train function\n","Now we define the train function that trains the model for an epoch."]},{"cell_type":"code","metadata":{"id":"ju-Co7ggY-aw"},"source":["def train():\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    for i, batch in enumerate(train_iter):\n","        data, target = batch.text, batch.target\n","        model.zero_grad()\n","        output = model(data)\n","        output = output.permute(0, 2, 1)\n","        loss = criterion(output, target)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        log_interval = 100\n","        if i % log_interval == 0 and i > 0:\n","            cur_loss = total_loss / log_interval\n","            elapsed = time.time() - start_time\n","            print(f'| epoch {epoch:3d} | {i:5d}/{len(train_iter):5d} batches | lr {lr:.4f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n","            total_loss = 0\n","            start_time = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg4MBanadLww"},"source":["### Validation function\n","\n","Now we will define the validation function, that given a dataset (val or test) will evaluate the loss of the prediction of the model in that dataset."]},{"cell_type":"code","metadata":{"id":"FUu9VNhzZBEU"},"source":["@torch.no_grad()\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0.\n","    n = 0\n","    for i, batch in enumerate(data_source):\n","        data, target = batch.text, batch.target\n","        output = model(data)\n","        output = output.permute(0, 2, 1)\n","        total_loss += target.numel() * criterion(output, target).item()\n","        n += target.numel()\n","    return total_loss / n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dKtCwcO8dOxA"},"source":["### Training loop\n","\n","This is the training loop code. At any point you can hit stop to get out of training early."]},{"cell_type":"code","metadata":{"id":"5snBY1x7OdHw"},"source":["best_val_loss = float(\"inf\")\n","\n","# At any point you can hit stop to get out of training early.\n","epochs = 4\n","try:\n","    for epoch in range(1, epochs+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(valid_iter)\n","        print('-' * 89)\n","        print(f'| end of epoch {epoch} | time: {(time.time() - epoch_start_time):.2f}s | valid loss {val_loss:.2f} | valid ppl {math.exp(val_loss):.2f}')\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if val_loss < best_val_loss:\n","            with open(\"best_checkpoint_transformer.pth\", 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        scheduler.step()\n","        lr = scheduler.get_last_lr()[0]\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(\"best_checkpoint_transformer.pth\", 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","\n","# Run on test data. \n","with torch.no_grad():\n","    test_loss = evaluate(test_iter)\n","print('=' * 89)\n","print(f'| End of training | test loss {test_loss:5.2f} | test ppl {math.exp(test_loss):8.2f}')\n","print('=' * 89)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gh17YMwgl1ei"},"source":["### Text generation"]},{"cell_type":"code","metadata":{"id":"8kxNIbY2PMgz"},"source":["model.eval()\n","\n","x = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n","temperature = 1  # Higher will increase diversity\n","text = \"\"\n","with torch.no_grad():\n","    for i in range(1000):\n","        output = model(x)\n","        word_weights = (output / temperature).exp().cpu().squeeze()  # Softmax without normalizing\n","        word_idx = torch.multinomial(word_weights, 1)[0]\n","        word_tensor = torch.tensor([[word_idx]], dtype=torch.long).to(device)\n","        x = torch.cat([x, word_tensor], dim=1)\n","        x = x[:, -35:]\n","        word = dataset.vocab.itos[word_idx]\n","        text += word + ('\\n' if i % 20 == 19  or word == '<eos>' else ' ')\n","\n","print(text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAIMWPxpRrhf"},"source":[""],"execution_count":null,"outputs":[]}]}