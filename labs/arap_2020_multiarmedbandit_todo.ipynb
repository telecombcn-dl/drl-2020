{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Multi-Armed Bandit.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OaQ_Q63YeDZe"},"source":["# Colab Notebook created for ARAP module at UPC ETSETB.\n","\n","**Authors**: Josep Vidal, Juan José Nieto, Margarita Cabrera-Bean\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"HYUIslJxfZuq","executionInfo":{"status":"ok","timestamp":1606561011220,"user_tz":-60,"elapsed":798,"user":{"displayName":"JuanJo Nieto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjgA4kOjCeh321lPGzT19jkCxELcD2KwBJElhGx72E=s64","userId":"18254924349580284490"}}},"source":["Student = '' #@param {type:\"string\"}"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UNJ2PtCELGqp"},"source":["# MULTI-ARMED BANDIT"]},{"cell_type":"markdown","metadata":{"id":"5prudN4jgrV3"},"source":["[Open task description in another tab](https://drive.google.com/file/d/10g5cJlfS_VcfEcrmdn4uNaRvEsSah8ax/view?usp=sharing)\n","\n","\n","Multi-armed bandit (MAB) framework has attracted a lot of attention in various applications, from recommender systems and information retrieval to healthcare and finance, due to its stellar performance combined with certain attractive properties, such as learning from less feedback. The multi-armed bandit field is currently flourishing, as novel problem settings and algorithms motivated by various practical applications are being introduced."]},{"cell_type":"markdown","metadata":{"id":"UZIVaSifgtrH"},"source":["Use Matlab or Python to solve next questions:\n","\n","\n","\n","*   Use the code provided for e-greedy algorithm. Check the reward on a single run for smaller values of the variance of the Gaussian f(r|a) in example 2.1 in slides. Extract conclusions.\n","*   Think of a practical application that can be modeled with an m-armed bandit. Guess a meaningful f(r|a) ∀a (Gaussian, binary Bernoulli, exponential, etc.) for that application. You may get some inspiration from [this paper](https://drive.google.com/file/d/1bOpOjRyHXAB91XF6uADV7tcWo2D2zHsK/view?usp=sharing). Assume stationarity over time.\n","*   Program it, use the base code provided, where Gaussian rewards have been assumed.\n","*   Check the average reward obtained in convergence for several values of the parameter associated to the UCB technique. Check also the number of correct decisions.\n","*   Plot the rewards for every action on a single run.\n","*   Make the environment non-stationary and include the appropriate changes.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"V0efF6QnEvG2"},"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evIvur2NNOx6"},"source":["np.random.seed(4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXmHYoL6f6X3"},"source":["### Plot functions"]},{"cell_type":"code","metadata":{"id":"4BaLYaK2f-oA"},"source":["'''Insert title, and axis labels to plots'''\n","def insert_labels(labels, ax):\n","    if 'title' in labels:\n","        ax.set_title(labels['title']) \n","    if 'xlabel' in labels:\n","        ax.set_xlabel(labels['xlabel']) \n","    if 'ylabel' in labels:\n","\t    ax.set_ylabel(labels['ylabel'])\n","\n","def generate_plot(r, ba, q, strg, par):\n","    str_legend = ['{}, {}={}'.format('UCB' if strg else 'e-greedy', 'c' if strg else 'delta', p) for p in par]\n","\n","    conf_1 = {'title': 'Average reward', 'xlabel': 'steps', 'ylabel': 'avg reward'}\n","    conf_2 = {'title': 'Percentage optimal actions', 'xlabel': 'steps', 'ylabel': '% optimal actions'}\n","    conf_3 = {'title': str_legend[-1], 'xlabel': 'steps', 'ylabel': 'Q(a)'}\n","\n","    fig, ax = plt.subplots(1,3, figsize=(18, 7))\n","    for i, (data, conf) in enumerate(zip([r, ba, q],[conf_1, conf_2, conf_3])):\n","        for j, d in enumerate(data):\n","            ax[i].plot(d)\n","            insert_labels(conf, ax[i])\n","        if i < 2:\n","            ax[i].legend(str_legend)\n","    \n","    ax[0].plot(np.ones(len(d))*np.max(meanA)) # Include the optimum reward in the display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjHCWTFYOI4_"},"source":["### Multi-armed bandit setting"]},{"cell_type":"code","metadata":{"id":"w1PAGdx5LNtB"},"source":["m = 10                                   # number of actions\n","dispMeansA = 1.5                        # dispersion in the values of means for every action\n","dispStd = 0.05                           # dispersion in the values of variances for every action\n","meanA = np.random.randn(m)*dispMeansA   # means for every action\n","bestAction = np.argmax(meanA)           # index of the best action\n","stdA = np.random.rand(m)*dispStd        # std deviations for every action\n","alpha = 0.01                            # time constant for incremental estimation of Q in time-varying environment"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jo-TpBP1OQjg"},"source":["### Simulation setting"]},{"cell_type":"code","metadata":{"id":"_nOmAxNWLucA"},"source":["NRuns = 200                             # number of independent runs to be averaged\n","NSteps = 500                            # number of time steps per run\n","\n","r = np.zeros((NRuns, NSteps))           # instantaneous rewards\n","Q = np.zeros((m, NSteps))               # average reward per action\n","BA = np.zeros((NRuns, NSteps))          # best action collection\n","\n","Strat = 0                               # 0: e-greedy, 1: UCB"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4vfmBYNKOT4Y"},"source":["### Decision taking setting"]},{"cell_type":"code","metadata":{"id":"iyIeE2LSODfH"},"source":["if Strat:\n","    par = np.array([0.5, 1, 2])        # values of c UCB                       \n","else:\n","    par = np.array([0, 0.1, 1])        # values of delta e-greedy       "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSQXNyPsnIZy"},"source":["### Random trials for each parameter of the algorithm and for all independent runs"]},{"cell_type":"code","metadata":{"id":"8CqJ4SQPQC6j"},"source":["avg_r = []\n","avg_ba = []\n","for e in range(len(par)):                                       # parameters for the method\n","    BA = np.zeros((NRuns, NSteps))                              # identifies if best action has been selected\n","    for i in range(NRuns):\n","        Q = np.zeros((m, NSteps))                               # average reward per action\n","        Q[:,0] = np.random.randn(m)*0.1                         # initialization of Q\n","        ta = np.zeros((m))                                      # times each action is selected\n","        for j in range(1, NSteps):\n","            # e-greedy\n","            if not Strat:\n","                I = np.argmax(Q[:,j-1])                         # select best action\n","                if np.random.rand() > min(1, m*par[e]/j):       # e-greedy with decaying epsilon\n","                    a = I\n","                else:\n","                    randIndex = np.random.randint(m-1)          # select an action other than greedy one\n","                    a = randIndex + (randIndex >= I)\n","                \n","                ta[a] += 1\n","                r[i,j] = meanA[a] + np.random.randn()*stdA[a]   # obtain the gaussian reward\n","                Q[:,j] = Q[:, j-1]                              # update Q function\n","                Q[a,j] += 1/ta[a] * (r[i,j] - Q[a, j])\n","            # UCB\n","            else:\n","                # COMPLETE THE CODE FOR UCB HERE\n","                pass # remove\n","\n","            BA[i,j] += bestAction == a\n","                \n","    avg_r.append(np.mean(r.copy(), axis=0))\n","    avg_ba.append(np.mean(BA.copy(), axis=0)*100)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5u7cHt1Fgffu"},"source":["generate_plot(avg_r, avg_ba, Q, Strat, par)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4BAyMzZp5sN"},"source":[""],"execution_count":null,"outputs":[]}]}