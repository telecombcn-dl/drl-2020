{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mrl_2020_07_reinforce_todo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0spTu6Dawi2S","colab_type":"text"},"source":["# Basic Policy Gradients (REINFORCE) example in PyTorch\n","\n","This notebook is an adaptation from the [PyTorch REINFORCE tutorial](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py).\n","\n","**Lab exercise prepared by [Víctor Campos](https://imatge.upc.edu/web/people/victor-campos), and adapted by [Juan José Nieto](https://www.linkedin.com/in/juan-jose-nieto-salas/) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).**\n","\n","![Víctor Campos](https://imatge.upc.edu/web/sites/default/files/styles/medium/public/users/vcampos//photo.jpg?itok=eCtqXNX9)\n","![Juan José Nieto](https://media-exp1.licdn.com/dms/image/C5603AQHLMgUe1Jvx-g/profile-displayphoto-shrink_200_200/0?e=1593648000&v=beta&t=AhGwoXIDMNNQj_2P6pFbp7RwD39PhpmpOU8OvOdRqC4)\n","![Xavier Giro-i-Nieto](https://telecombcn-dl.github.io/2019-dlcv/img/instructors/XavierGiro-160x160.jpg)"]},{"cell_type":"markdown","metadata":{"id":"hIYC7IeEwrlt","colab_type":"text"},"source":["## Installing dependencies\n","\n","We will use OpenAI Gym to simulate the environment, which might not be installed by default. We also need to install some dependencies for visualization purposes (this may take a while)."]},{"cell_type":"code","metadata":{"id":"Pm4IZndds2u2","colab_type":"code","colab":{}},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzOcEmlJw6gb","colab_type":"text"},"source":["## Setting up the environment\n","\n","We will need some tricks to visualize the simulations in the browser, as simply calling env.render() will not work in this notebook ([source](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=Jyb2Ujuozfi2&forceEdit=true&offline=true&sandboxMode=true))."]},{"cell_type":"code","metadata":{"id":"cElJ0d4PsX5X","colab_type":"code","colab":{}},"source":["import gym\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHiHbaL85e1k","colab_type":"code","colab":{}},"source":["from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DmTRRyhmgYt","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnf0NPZ_ss8u","colab_type":"code","colab":{}},"source":["print('PyTorch version: ', torch.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUlPxFV1xtw4","colab_type":"code","colab":{}},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_b9tRuVxwP3","colab_type":"code","colab":{}},"source":["def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBKofZ4KrG8Z","colab_type":"text"},"source":["## Visualize a random policy in the environment\n","\n","Our goal is to train an agent that is capable of solving the CartPole problem, where a pole is attached to a cart moving along a horizontal track. The agent can interact with the environment by applying a force (+1/-1) to the cart. The episode is terminated whenever the pole is more than 15 degrees from vertical or the cart goes out of bounds in the horizontal axis. The agent receives +1 reward for each timestep under the desired conditions.\n","\n","We can visualize what a random policy would do in this environment:"]},{"cell_type":"code","metadata":{"id":"mH913G8v01dn","colab_type":"code","colab":{}},"source":["# Let's generate a random trajectory...\n","env = wrap_env(gym.make(\"CartPole-v1\"))\n","ob, done, total_rew = env.reset(), False, 0\n","while not done:\n","  env.render()\n","  ac = env.action_space.sample()\n","  ob, rew, done, info = env.step(ac)\n","  total_rew += rew\n","print('Cumulative reward:', total_rew)\n","  \n","# ... and visualize it!\n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeZXG6VHygrD","colab_type":"text"},"source":["## Create the model\n","\n","Now we will define our policy, parameterized by a feedforward neural network.\n","\n","**Exercise #1.** Implement the policy as an MLP with a hidden layer of 128 neurons with a ReLU activation and a Softmax output layer."]},{"cell_type":"code","metadata":{"id":"jKr_lu19yc3B","colab_type":"code","colab":{}},"source":["class Policy(nn.Module):\n","    def __init__(self, inputs, outputs):\n","        super(Policy, self).__init__()\n","        # TODO\n","        self.affine1 = nn.TODO\n","        self.affine2 = nn.TODO\n","\n","        self.saved_log_probs = []\n","        self.rewards = []\n","\n","    def forward(self, x):\n","        # TODO\n","        x = self.TODO\n","        x = F.TODO\n","        action_scores = self.TODO\n","        return F.softmax(action_scores, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0T02F2uyndg5","colab_type":"text"},"source":["## Functions for collecting experience and updating the policy\n","\n","**Exercise #2.** Use the policy to predict a distribution of probabilities across actions.\n","\n","**Exercise #3.** Compute the return from the rewards collected by the policy.\n","\n","**Exercise #4.** Complete the loss computation using the returns and the log probs."]},{"cell_type":"code","metadata":{"id":"lcXYBZNznd96","colab_type":"code","colab":{}},"source":["def select_action(policy, state):\n","    # Convert state into PyTorch tensor\n","    state = torch.from_numpy(state).float().unsqueeze(0)\n","    # TODO: Compute action probabilities\n","    probs = TODO\n","    # Sample action\n","    m = torch.distributions.Categorical(probs)\n","    action = m.sample()\n","    # Bookkeeping\n","    policy.saved_log_probs.append(m.log_prob(action))\n","    return action.item()\n","\n","\n","def train(policy, optimizer):\n","    R = 0\n","    policy_loss = []\n","    returns = []\n","    # Compute the returns by reading the rewards vector backwards\n","    for r in policy.rewards[::-1]:\n","        # TODO: Complete the computation of the return using gamma\n","        R = r + TODO\n","        returns.insert(0, R)\n","    returns = torch.tensor(returns)\n","    # Normalize returns (this usually accelerates convergence)\n","    returns = (returns - returns.mean()) / (returns.std() + eps)\n","    for log_prob, R in zip(policy.saved_log_probs, returns):\n","        # TODO: Complete the 'loss' computation using the returns and the log probs.\n","        policy_loss.append(TODO)\n","    # Update policy: \n","    #  (1) reset optimizer grads\n","    optimizer.zero_grad()\n","    #  (2) compute surrogate policy gradients loss\n","    policy_loss = torch.cat(policy_loss).sum()\n","    #  (3) SGD step\n","    policy_loss.backward()\n","    optimizer.step()\n","    del policy.rewards[:]\n","    del policy.saved_log_probs[:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr4pSznznosA","colab_type":"text"},"source":["## Training the agent"]},{"cell_type":"code","metadata":{"id":"0tJ7RcgfnxPa","colab_type":"code","colab":{}},"source":["# Hyperparameters\n","env_name = 'CartPole-v1'\n","gamma = 0.99  # discount factor\n","seed = 543  # random seed\n","log_interval = 10  # controls how often we log progress\n","max_ep_len = 1000  # maximum episode length\n","num_episodes = 1500  # number of episodes to train on"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLb1oPuGoJpJ","colab_type":"code","colab":{}},"source":["# Create environment\n","env = gym.make(env_name)\n","\n","# Fix random seed (for reproducibility)\n","env.seed(seed)\n","torch.manual_seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AGdVUq4nrWg","colab_type":"code","colab":{}},"source":["# Create policy and optimizer\n","n_inputs = env.observation_space.shape[0]\n","n_actions = env.action_space.n\n","policy = Policy(n_inputs, n_actions)\n","optimizer = torch.optim.Adam(policy.parameters(), lr=1e-2)\n","eps = np.finfo(np.float32).eps.item()\n","\n","\n","# Training loop\n","print(\"Target reward: {}\".format(env.spec.reward_threshold))\n","running_reward = 10\n","ep_rew_history = []\n","for i_episode in range(num_episodes):\n","    # Collect experience\n","    state, ep_reward = env.reset(), 0\n","    for t in range(max_ep_len):  # Don't infinite loop while learning\n","        \n","        action = select_action(policy, state)\n","        state, reward, done, _ = env.step(action)\n","        policy.rewards.append(reward)\n","        ep_reward += reward\n","        if done:\n","            break\n","\n","    # Update running reward\n","    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n","    \n","    # Perform training step\n","    train(policy, optimizer)\n","    ep_rew_history.append((i_episode, ep_reward))\n","    if i_episode % log_interval == 0:\n","        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n","              i_episode, ep_reward, running_reward))\n","    if running_reward > env.spec.reward_threshold:\n","        print(\"Solved!\")\n","        break\n","\n","print(\"Finished training! Running reward is now {:.2f} and \"\n","      \"the last episode runs to {} time steps!\".format(running_reward, t))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mv90goqpVguY","colab_type":"code","colab":{}},"source":["plt.plot([x[0] for x in ep_rew_history], [x[1] for x in ep_rew_history])\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cESm8xBnFj-","colab_type":"text"},"source":["## Visualize trained policy"]},{"cell_type":"code","metadata":{"id":"F97G8ZBUyPAN","colab_type":"code","colab":{}},"source":["test_env = wrap_env(gym.make(env_name))\n","state, ep_reward, done = test_env.reset(), 0, False\n","while not done:\n","    test_env.render()\n","    action = select_action(policy, state)\n","    state, reward, done, _ = test_env.step(action)\n","    ep_reward += reward\n","print(\"Cumulative reward: {}\".format(ep_reward))\n","\n","test_env.close()\n","show_video()"],"execution_count":0,"outputs":[]}]}