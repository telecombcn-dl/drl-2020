{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"arap_2020_lab_5_gcn_todo.ipynb","provenance":[{"file_id":"1_3uAYmUsl0ZtdXrvAtboVNYy7nOTGFvH","timestamp":1591291213113}],"collapsed_sections":["tFvFU6xnMazT","sb64LkymNuw0","rdBuABk6wUVE"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_sCPCyEgzl4d"},"source":["# Graph Convolutional Neural Networks\n","\n","\n","In this notebook will learn about Graph Convolutional Neural Networks by implementing a simple GCN to classify the nodes of a graph.\n","\n","**Important:** Set the Colab environment to run on GPU\n","\n","**Notebook created by Paula Gómez Duran**\n"]},{"cell_type":"markdown","metadata":{"id":"iHcRKrNEbGNk"},"source":["## **Installation . . .**"]},{"cell_type":"code","metadata":{"id":"PeL_71l3anuS"},"source":["%tensorflow_version 1.x\n","import torch\n","if not torch.cuda.is_available():\n","    raise Exception(\"You should enable GPU runtime\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_u6Sbt3lR-h"},"source":["!pip install -q torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n","!pip install -q torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n","!pip install -q git+https://github.com/rusty1s/pytorch_geometric.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFvFU6xnMazT"},"source":["## **Understanding GCN concept by building a simple graph . . .**\n","\n","<div>\n","<center><img src=\"https://miro.medium.com/max/864/1*jTW7doI_cqC_p9XQrmuu9A.png\" width=\"400\"/></center>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"nPR4n8eZNJSj"},"source":["### Defining the graph and it's features\n","\n","So, we build the adjacency matrix A from the previous figure:"]},{"cell_type":"code","metadata":{"id":"cJ8ERA93MaLF"},"source":["import numpy as np\n","#############################\n","# Exercice 1: Complete the last two rows of the matrix. Will or will not be symmetric matrix?\n","#############################\n","A = np.matrix([\n","    [0, 1, 0, 0],\n","    [0, 0, 1, 1], \n","    # [ ... ],\n","    # [ ... ]],\n","    dtype=float\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cD7CuBZLNSeN"},"source":["Now, we generate 2 features for each node based on the index. This makes it easy to confirm the matrix calculations manually later."]},{"cell_type":"code","metadata":{"id":"aX1OI2YLMaaM"},"source":["X = np.matrix([\n","            [i, -i]\n","            for i in range(A.shape[0])\n","        ], dtype=float)\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sb64LkymNuw0"},"source":["### Applying the simple propagation rule for the input layer:\n","\n","Now we will try to implement the layer equation defined in theory. To make it easier, we will ommit the normalization part, as it is implicit in the implementation of the model from Pytorch Geometric that we will use and it is not necessary to get the intuition of how GCN works.\n","\n","\n","### ` f(Hⁱ, A) = σ(A Hⁱ Wⁱ) `\n","\n","*   H⁰ = X = input features\n","*   AH⁰W⁰ = AXW⁰ = AX\n","*   W = I\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"7T9Z-KW5W5Vc"},"source":["H_0 = X "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sgLS-8DKW_Lh"},"source":["W = np.identity(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAo1Tq_-NOFd"},"source":["H_1 = A * H_0 * W\n","H_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oAj8_j7IPlzl"},"source":["As you can observe, the representation of each node (each row) is now a sum of its neighbors features! \n","\n","In other words, the graph convolutional layer represents each node as an aggregate of its neighborhood, as we saw on theory slides."]},{"cell_type":"code","metadata":{"id":"kzKee-LSa71e"},"source":["for i in range(1000):\n","    X = A * X * W\n","print(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mu4u-0B-iLNG"},"source":["### Adding self loops\n","\n","If you look carefuly to the previous representation of each node after the popagation rule, the node itself is not taken into account. Thus, that is the reason why we need to add self loops to the adjacency matrix, conforming the Â matrix.\n"]},{"cell_type":"code","metadata":{"id":"jWW3-f4Bh7li"},"source":["#############################\n","# Exercice 2: Build identity matrix I using np.identity() \n","#             Sum I with A matrix to build Â\n","#############################\n","\n","I  = # ... \n","I"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frBdysYdhqtr"},"source":["A_hat = A + I\n","A_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVQQd5p8iqcQ"},"source":["# Now run again the GCN equation and observe the new output\n","H_1 = A_hat * H_0 * W\n","H_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BX1AbF7PBumV"},"source":["As you observe, now we have taken into account the node itself when computing the aggregation of local neightbours by A * X."]},{"cell_type":"markdown","metadata":{"id":"aJANCxZ9oWne"},"source":["### Why do we need to normalize?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZzPjFygeqoaJ"},"source":["Here we intent to show what happens without normalizing. Then, we will show whats the same output when applying normalization.\n","\n","> So, lets try to apply the GCN equation for 1000 epochs:"]},{"cell_type":"code","metadata":{"id":"vIumSflknxr_"},"source":["H_n = H_1\n","for i in range(1000):\n","    H_n = A_hat * H_n * W\n","\n","print(H_n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CUsqSu7vPJi"},"source":["WHY DO YOU THINK IT HAPPENS?"]},{"cell_type":"markdown","metadata":{"id":"OwqZOx7JrLzV"},"source":["> You can observe that the output features for epoch 1000 are nan... So, let's try to normalize before and repeat the operation."]},{"cell_type":"code","metadata":{"id":"1sNBE1Z4odxJ"},"source":["A_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6UOQAEypF57"},"source":["D = np.array(A_hat.sum(1))\n","D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVzhrzHZpyLU"},"source":["D_inv_half = np.power(D, -0.5).flatten()\n","D_mat = np.diag(D_inv_half)\n","# SYMMETRIC NORMALIZATION = D^-1/2 * (H) * D^-1/2\n","aux = D_mat.dot(A_hat)\n","norm_A_hat = aux.dot(D_mat)\n","norm_A_hat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1Z5_OrcbSIF"},"source":["H_1_norm = norm_A_hat * H_0 * W\n","H_1_norm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpcQUWqOn2QI"},"source":["H_n = H_1_norm\n","for i in range(1000):\n","    H_n = norm_A_hat * H_n * W\n","\n","print(H_n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MvilX2FNrZ3F"},"source":["> Now, as you observe we still can get good results after iterating for 1000 epochs. \n","\n","\n","The explanation is that normalizing avoid numbers to be higher than one in all the multiplications, and so we need to do it in order to solve possible problems such as exploding gradients or having weights which are too big."]},{"cell_type":"markdown","metadata":{"id":"rdBuABk6wUVE"},"source":["## **Moving to a real challenge . . .**\n","\n","Now, we will explore how to solve a classification problem with GCN. \n"]},{"cell_type":"markdown","metadata":{"id":"9GQiPS4vQDri"},"source":["\n","\n","The task that we will solve is based on **Cora dataset** ( [link here](https://relational.fit.cvut.cz/dataset/CORA) ), which consists of 2708 scientific publications classified into seven classes:\n","\n","\t\tCase_Based\n","\t\tGenetic_Algorithms\n","\t\tNeural_Networks\n","\t\tProbabilistic_Methods\n","\t\tReinforcement_Learning\n","\t\tRule_Learning\n","\t\tTheory\n","\n","The papers were selected in a way such that every paper cites or is cited by atleast one other paper. The citation network consists of 5429 links which represents the edge of the graph. Therefore, we can assume that the data is describing in a graph structure.\n","\n","Besides, the dataset also provides a 0/1-valued word vector for each of the papers which indicates the absence/presence of each corresponding word from the dictionary, which has a lenght of 1433 positions.\n","\n","So, as features of each node we will have a vector of binary values indicating whether each word in the vocabulary is present (indicated by 1) or absent (indicated by 0) in the paper. I have put an example here for a better understanding of the concept:\n","\n","\n","<div>\n","<center><img src=\"https://miro.medium.com/max/906/1*f5e9vn4EZB8zNSLWO0dn-A.png\"/></center>\n","</div>\n","\n","\n","So, assuming that the dictionary consists in those words (awesome, funny, hate, it ...), we would have for each of the papers (0, 1, 2, 3 in the example above), a 1 value in those positions where the corresponding word was present.\n","\n","\n","> **About dictionary:** After stemming and removing stopwords we were left with a vocabulary of size 1433 unique words. All words with document frequency less than 10 were removed.\n","\n","\n","Finally, they also provide labels for each of the papers which corresponds to a category that indicates the class of which the paper has been classified on.\n","\n"," &nbsp;\n","\n","**The goal is to classify each document into one of the seven classes**. In order to make it easier when working with graphs we will use the library **Pytorch Geometric**. You can find the documentation [here](https://pytorch-geometric.readthedocs.io/en/latest/) and it's github page [here](https://github.com/rusty1s/pytorch_geometric).\n","\n"," &nbsp;\n"," "]},{"cell_type":"markdown","metadata":{"id":"nGJ2Lyqa8GbG"},"source":["Now, we are going to import [**Tensorboard**](https://www.tensorflow.org/tensorboard/get_started), which is a tool for providing the measurements and visualizations needed during the machine learning workflow. It is very easy to use and it will allow us to see straightforward the behaviour of our network."]},{"cell_type":"code","metadata":{"id":"aAUPGJrHXqfM"},"source":["%tensorflow_version 1.x\n","from tensorboardcolab import TensorBoardColab\n","tbc = TensorBoardColab()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyCVAY9ATbOx"},"source":["Prepare the necessary imports:"]},{"cell_type":"code","metadata":{"id":"0X3R9ymWQTAu"},"source":["# Write code for imports\n","import os.path as osp\n","import argparse\n","import numpy as np\n","\n","import torch\n","import torch.nn.functional as F\n","from torch_geometric.datasets import Planetoid\n","import torch_geometric.transforms as T\n","from torch_geometric.nn import GCNConv, ChebConv  \n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7Pja2eNYQBx"},"source":["\n","\n","Pytorch Geometric already implements **Planetoid** class, which loads **Cora** dataset into a very useful way for using GCN. Let's take a look!"]},{"cell_type":"code","metadata":{"id":"Flr8-B7OQTJ4"},"source":["dataset_name = 'Cora'\n","path = osp.join(osp.dirname(osp.realpath('coradataset')), 'data', dataset_name)\n","# NormalizeFeatures --> Row-normalizes node features to sum-up to one.\n","dataset = Planetoid(path, dataset_name, transform=T.NormalizeFeatures())\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhA3bTHTQTRQ"},"source":["# Write code for visualizing dataset object\n","data = dataset[0]\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEKsG6w0STNO"},"source":["dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhA2RH_6nONW"},"source":["#### Understanding data...\n","\n","As we previously said, Cora dataset contains bag-of-words representation of documents and citation links between the documents. Just above we can observe the attributs of the [*InMemoryDataset (Planetoid)*](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/planetoid.html#Planetoid) class from Pytorch Geometric library that will help us. Let's analyse each of them!\n","\n","&nbsp;\n","\n","We will treat the bag-of-words vectors as feature vectors **X**. As we can observe, the shape of *X* is the number of nodes on the rows and the lenght of the dicctonaty on the columns. So, we have a matrix which contains a feature vector for each of the nodes (one representation in each row).\n"]},{"cell_type":"code","metadata":{"id":"4dI-ozkMh7hw"},"source":["#############################\n","# Exercice 3: Write code for visualizing the shape of the input node features[Num_nodes, Dicctionary lenght]\n","#             TIP: take into account that `dataset` variable is different from `data` object. You might want to check on `data`.            \n","#############################\n","\n","# ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3wCxpEj6OhU"},"source":["# Visualize first feature vector (feature of node 0)\n","\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nw_LxN6vx5B4"},"source":["To construct the graph, we need to build the adjacency matrix *A* based on the citation links: if document *i* cites *j*, then we set *aij* = *aji* = 1. However, Pytorch Geometric gives us those relationships in the *edge_index* variable, which denotes each of the positive interactions of A. \n","\n","To make it clear, **A** will have shape (2708, 2708) in order to have both in rows and in columns all the nodes and so be able to set to 1 those positions where document *i* cited document *j*. The next variable **edge_index** will refer to all the positions set to one: \n","\n","      node 0 - node 633\n","      node 0 - node 1862\n","      node 0 - node 2582\n","              .\n","              .\n","              .\n","     node 2707 - node 1473\n","     node 2707 - node 2706\n","\n"]},{"cell_type":"code","metadata":{"id":"YV_IfmnWntgV"},"source":["# Print edge_index variable \n","data.edge_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19VOJd8mlLR7"},"source":["If we check with how many nodes (papers) is related a given paper, we can observe that still there is a lot more which with it is not related. Thus, the matrix **A** will be a sparse matrix, which is basically a matrix contaning a lot of zeros. \n"]},{"cell_type":"code","metadata":{"id":"E4TMSyCW9fdV"},"source":["# Print number of connections per node\n","for node in [1, 2, 3]:\n","    print(f'Number of connections for node {node} : {np.where(data.edge_index[0].numpy() == node)[0]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LBpaiPGr_iG8"},"source":["So, if you see above, the node 1 is connected with 3 nodes; the node 2 with 5 nodes; and the node 3 with one node. Thus, we can confirm that the majory of connections of **A** will be set to 0 - being **A** a sparse matrix."]},{"cell_type":"markdown","metadata":{"id":"ap7MeZ_W_U3t"},"source":["\n","> **Remember:** The diagonal of **A** will always be full of zeros before adding the self-loops (identity matrix)."]},{"cell_type":"markdown","metadata":{"id":"GECElY4Q01OF"},"source":["#### SPLIT\n","If we look at *Planetoid class* [documentation](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/planetoid.html#Planetoid) that corresponds to the dataset we are using, we will see that there is a **split** flag where we can decide the way the dataset should be splitted. The library provides us three different ways to do it, as it is explained in [docs](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/planetoid.html#Planetoid):\n","\n","\n","* \"public\" : the split will be the public fixed split from the \"Revisiting Semi-Supervised Learning with Graph Embeddings\" [paper](https://arxiv.org/abs/1603.08861).\n","\n","* \"full\": all nodes except those in the validation and test sets will be used for training, as in the \"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\" [paper](https://arxiv.org/abs/1801.10247).\n","\n","* \"random\": train, validation, and test sets will be randomly generated, according to `num_train_per_class`, `num_val` and `num_test`. \n","\n","\n","The default value of the split is the *public* one and so what the dataset returns are three different masks: one for training, one for validation and one for test. Each of the mask will have a true those indices selected for either training, validation or test.\n","\n"]},{"cell_type":"code","metadata":{"id":"BYfeUH6UgDkY"},"source":["# Print the mask for training set\n","data.train_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVb3Sl4x1cOV"},"source":["# Check training labels\n","data.y[data.train_mask]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bfhomE2H__Ap"},"source":["\n","We will let it set as the default option, which refers to the public split on their paper **\"Revisiting Semi-Supervised Learning with Graph Embeddings\"**. It consists in randomly sample 20 instances for each class as labeled data (20*7 = 140) and then evaluate it on 1000 instances as test data. The rest of data will be used for validation as unlabeled data. "]},{"cell_type":"code","metadata":{"id":"90kGhNdTmqQb"},"source":["# Check how many labels we have\n","\n","len(data.y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ugfw25_mubd"},"source":["# Print lenght of training, val and test samples\n","print(len(data.y[data.train_mask]), len(data.y[data.val_mask]), len(data.y[data.test_mask]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8awPQKjK2MT4"},"source":["In case you did not notice, we imported *GCNConv class* above and you can look at it's documentation [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=GCNConv#torch_geometric.nn.conv.GCNConv). **GCNConv** implements the GCN operation we explained in the slides, which is basically a graph convolutional layer. To build the network, we will use two GCN layers thus combining them with some activation functions and dropout regularization. In the end, we will use a *log_softmax* function to get the class prediction probabilities. \n","\n","\n","You can read the documentation for better understanding how this class works and what it is already implementing."]},{"cell_type":"code","metadata":{"id":"8e_8voTGESdz"},"source":["# Build GCN model\n","#############################\n","# Exercice 4: Build two GCNConv layers that go from:\n","#             1. the number of features to hidden dimension\n","#             2. hidden dimension to number of classes\n","# Set cached = True for both layers\n","#############################\n","\n","class GCN_network(torch.nn.Module):\n","    def __init__(self, num_features, num_classes, hidden_dim=16):\n","        super(GCN_network, self).__init__()\n","        self.conv1 = # ...\n","        self.conv2 = # ...\n","        \n","    def forward(self, data):\n","        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n","        \n","        x = F.relu(self.conv1(x, edge_index, edge_weight))\n","        # DEFAULT DO = 0.5\n","        x = F.dropout(x, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","\n","        return F.log_softmax(x, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1FUcvcnB3Lnr"},"source":["Now we will declare the network as well as the optimizer with the needed parameters."]},{"cell_type":"code","metadata":{"id":"LesstYSVEC2_"},"source":["# Declare model and optimizer. Also, move data to device.\n","\n","model = GCN_network(dataset.num_features, dataset.num_classes).to(device)\n","data = data.to(device) \n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"un0-wk92AYJ6"},"source":["In Graphs, we do not have three different datasets (three different graphs). However, we have one entire graph and then the labels are boolean masks which allow us to select the nodes. Thus, the way to compute the predictions will be to predict the whole graph and select the indexes that we want either for train, val or test depending on the situation."]},{"cell_type":"code","metadata":{"id":"__j43_Uh45ew"},"source":["# Predict for all data and check shape\n","\n","entire_graph_pred = model(data)\n","entire_graph_pred.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuaRkjEL5Yp-"},"source":["# Select just training set predictions\n","\n","entire_graph_pred[data.train_mask].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8yApM3FA0vb"},"source":["Now we proceed to build the **train** and **validation** functions for training the network and evaluating it. Besides, we will also build the **test** function for performing inference with **accuracy** metric in each of the sets."]},{"cell_type":"code","metadata":{"id":"xEMttI1YEms4"},"source":["# Write code for tranining function\n","\n","def train():\n","    model.train()\n","    optimizer.zero_grad()\n","    train_pred = model(data)[data.train_mask]\n","    train_labels = data.y[data.train_mask]\n","\n","    train_loss = F.nll_loss(train_pred, train_labels)\n","    train_loss.backward()\n","    tbc.save_value('loss/train', 'train_loss', epoch, train_loss.item())\n","    optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ajSJtW06_rR"},"source":["# Write code for validation function\n","\n","@torch.no_grad()\n","def validation():\n","    model.eval()\n","    #############################\n","    # Exercice 5: Take the validation indices of the entire graph predictions\n","    #############################\n","    \n","    val_pred = # ...\n","    val_labels = data.y[data.val_mask]\n","\n","    val_loss = F.nll_loss(val_pred, val_labels)\n","    tbc.save_value('loss/val', 'val_loss', epoch, val_loss.item())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZ0alfIpEvg6"},"source":["# Write code for evaluation function\n","@torch.no_grad()\n","def test():\n","    model.eval()\n","    logits = model(data)\n","    accs = []\n","    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n","\n","        pred = logits[mask].max(1)[1]\n","        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n","        accs.append(acc)\n","    return accs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WtTDxTuHBJvc"},"source":["As the last stage, we will write the code for the entire workflow for 300 epochs.\n","\n","1. Training\n","2. Validating\n","3. Inference"]},{"cell_type":"code","metadata":{"id":"wJ8tmCeXFGEa"},"source":["best_val_acc = test_acc = 0\n","for epoch in range(1, 301):\n","    train()\n","    validation()\n","    train_acc, val_acc, tmp_test_acc = test()\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        test_acc = tmp_test_acc\n","    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n","\n","    tbc.save_value('acc/train','train_acc', epoch, train_acc)\n","    tbc.save_value('acc/val','val_acc', epoch, best_val_acc)\n","    tbc.save_value('acc/test','test_acc', epoch, test_acc)\n","\n","    print(log.format(epoch, train_acc, best_val_acc, test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vEIbC7W0BqXe"},"source":["Now we have trained the network and evaluated it! You can go to the Tensorboard link generated above and you will be able to see that actually, we did not need as much epochs because the model is overfitting in approximately epoch 90 (validation loss stops going down). Even though, we have achieved a pretty good performance on test accuracy (0.7910). \n"]}]}