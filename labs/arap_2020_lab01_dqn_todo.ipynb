{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"arap_2020_lab01_dqn_todo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Mh3KayG0qaJw"},"source":["Notebook created by [Víctor Campos](https://imatge.upc.edu/web/people/victor-campos) for UPC ETSETB AAL 2019\n","\n","Updates:\n","\n","[Xavier Giró](https://imatge.upc.edu/web/people/xavier-giro) - UPC ETSETB AAL 2019"]},{"cell_type":"markdown","metadata":{"id":"0spTu6Dawi2S"},"source":["# DQN example in PyTorch\n","\n","This notebook is adapted from the [official DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). Unlike the tutorial, we will use the standard observation instead of the RGB images."]},{"cell_type":"code","metadata":{"id":"Y6HRPJ_4ioex","cellView":"form"},"source":["#@title Train/eval selection\n","#@markdown Sometimes the notebook will crash when running an evaluation rollout (with render=True) after training. If this happens, you can skip training and load a pre-trained model with the checkboxes below.\n","\n","train_model = True #@param {type:\"boolean\"}\n","load_model = False #@param {type:\"boolean\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hIYC7IeEwrlt"},"source":["## Installing dependencies\n","\n","We will use OpenAI Gym to simulate the environment, which might not be installed by default. We also need to install some dependencies for visualization purposes (this may take a while)."]},{"cell_type":"code","metadata":{"id":"Pm4IZndds2u2"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzOcEmlJw6gb"},"source":["## Setting up the environment\n","\n","We will need some tricks to visualize the simulations in the browser, as simply calling env.render() will not work in this notebook ([source](https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=Jyb2Ujuozfi2&forceEdit=true&offline=true&sandboxMode=true))."]},{"cell_type":"code","metadata":{"id":"cElJ0d4PsX5X"},"source":["import os\n","import gym\n","import time\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import namedtuple"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pHiHbaL85e1k"},"source":["from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DmTRRyhmgYt"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# if gpu is to be used\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nnf0NPZ_ss8u"},"source":["print('PyTorch version: ', torch.__version__)\n","print('Using GPU:', ['no', 'yes'][int(torch.cuda.is_available())])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUlPxFV1xtw4"},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_b9tRuVxwP3"},"source":["def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBKofZ4KrG8Z"},"source":["## Visualize a random policy in the environment\n","\n","Our goal is to train an agent that is capable of solving the CartPole problem, where a pole is attached to a cart moving along a horizontal track. The agent can interact with the environment by applying a force (+1/-1) to the cart. The episode is terminated whenever the pole is more than 15 degrees from vertical or the cart goes out of bounds in the horizontal axis. The agent receives +1 reward for each timestep under the desired conditions.\n","\n","**Exercise #1.** Visualize a rollout of a random agent in the [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/) environment. Use the [documentation](http://gym.openai.com/docs/) for OpenAI Gym as a reference."]},{"cell_type":"code","metadata":{"id":"mH913G8v01dn"},"source":["# Let's generate a random trajectory...\n","\n","# Choose the Cart-Pole environment from the OpenAI Gym\n","env = wrap_env(gym.make(\"CartPole-v1\"))\n","\n","# Initialize the variables ob (observation=state), done (breaks loop) \n","# and total_rew (reward)\n","ob, done, total_rew = env.TODO, False, 0\n","\n","# Execution loop\n","while not done:\n","  env.render()\n","  \n","  # Sample a random action from the environment\n","  ac = env.TODO\n","  \n","  # Obtain the new state, reward and whether the episode has terminated\n","  ob, rew, done, info = env.TODO\n","  \n","  # Accumulate the reward\n","  total_rew += rew\n","  \n","print('Cumulative reward:', total_rew)\n","  \n","# ... and visualize it!\n","env.close()\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5XIB8KMAw1HL"},"source":["## Replay memory\n","\n","The buffer will be a FIFO queue: when full, oldest experiences are removed to make room for new transitions.\n","\n","**Exercise #2.** Implement the pointer to the next position to be filled in the replay memory, which corresponds to a FIFO queue.\n","(TIP: remember the [modulus % operator](https://python-reference.readthedocs.io/en/latest/docs/operators/modulus.html))."]},{"cell_type":"code","metadata":{"id":"DOxQfeRzw30X"},"source":["Transition = namedtuple(\n","    'Transition', ('state', 'action', 'next_state', 'reward'))\n","\n","\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, *args):\n","        \"\"\"Saves a transition.\"\"\"\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)\n","        self.memory[self.position] = Transition(*args)\n","        # TODO: Update the pointer to the next position in the replay memory\n","        self.position = TODO\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HeZXG6VHygrD"},"source":["## Create the model\n","\n","Now we will define our policy, parameterized by a feedforward neural network.\n","\n","**Exercise #3.** Complete the forward method of a [neural network in PyTorch](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) by composing the two linear layer of perceptrons with a ReLU activation in between."]},{"cell_type":"code","metadata":{"id":"jKr_lu19yc3B"},"source":["class DQN(nn.Module):\n","    def __init__(self, inputs, outputs, hidden_size=128):\n","        super(DQN, self).__init__()\n","        self.affine1 = nn.Linear(inputs, hidden_size)\n","        self.affine2 = nn.Linear(hidden_size, outputs)\n","\n","    # Called with either one element to determine next action, or a batch\n","    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n","    # TODO: Define the operations in the forward pass. \n","    def forward(self, x):\n","        x = self.TODO\n","        x = F.relu(x)\n","        x = self.TODO\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0T02F2uyndg5"},"source":["## Functions for collecting experience and updating the policy\n","\n","**Exercise #4.** Complete eps_greedy policy to facilitate the exploration.\n","\n","**Exercise #5.** Complete with `policy_net` or `target_net` the `TODO_net` in the code.\n"]},{"cell_type":"code","metadata":{"id":"lcXYBZNznd96"},"source":["def compute_eps_threshold(step, eps_start, eps_end, eps_decay):\n","  return eps_end + (eps_start - eps_end) * math.exp(-1. * step / eps_decay)\n","\n","\n","def select_action(policy, state, eps_greedy_threshold, n_actions):\n","    # TODO: Select action using an epsilon-greedy strategy\n","    if random.random() > TODO:\n","      with torch.no_grad():\n","            # t.max(1) will return largest column value of each row.\n","            # second column on max result is index of where max element was\n","            # found, so we pick action with the larger expected reward.\n","            action = policy(state).max(1)[1].view(1, 1)\n","    else:\n","      action = torch.tensor(\n","          [[random.randrange(n_actions)]], device=device, dtype=torch.long)\n","    return action\n","\n","    \n","def train(policy_net, target_net, optimizer, memory, batch_size, gamma):\n","    if len(memory) < batch_size:\n","        return\n","    transitions = memory.sample(batch_size)\n","    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","    # detailed explanation). This converts batch-array of Transitions\n","    # to Transition of batch-arrays.\n","    batch = Transition(*zip(*transitions))\n","\n","    # Compute a mask of non-final states and concatenate the batch elements\n","    # (a final state would've been the one after which simulation ended)\n","    non_final_mask = torch.tensor(\n","        tuple(map(lambda s: s is not None, batch.next_state)), \n","        device=device, \n","        dtype=torch.bool)\n","    non_final_next_states = torch.cat(\n","        [s for s in batch.next_state if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    # Compute Q(s_t, a) - the model computes Q(s_t) for all a, then we select \n","    # the columns of actions taken.\n","    state_action_values = TODO_net(state_batch).gather(1, action_batch)\n","\n","    # Compute Q(s_{t+1}) for all next states.\n","    # Expected values of actions for non_final_next_states are computed based\n","    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","    # This is merged based on the mask, such that we'll have either the expected\n","    # state value or 0 in case the state was final.\n","    # Note the call to detach() on Q(s_{t+1}), which prevents gradient flow\n","    next_state_values = torch.zeros(batch_size, device=device)\n","    next_state_values[non_final_mask] = TODO_net(\n","        non_final_next_states).max(1)[0].detach()\n","        \n","    # Compute targets for Q values: y_t = r_t + max(Q_{t+1})\n","    expected_state_action_values = reward_batch + (next_state_values * gamma)\n","\n","    # Compute Pseudo-Huber loss between predicted Q values and targets y\n","    loss = F.smooth_l1_loss(\n","        state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","    # Take an SGD step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    \n","def test(env, policy, render=False):\n","    state, ep_reward, done = env.reset(), 0, False\n","    while not done:\n","        if render:\n","          env.render()\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","        action = select_action(policy_net, state, 0., 1)\n","        state, reward, done, _ = env.step(action.item())\n","        ep_reward += reward\n","    return ep_reward"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr4pSznznosA"},"source":["## Training the agent"]},{"cell_type":"code","metadata":{"id":"0tJ7RcgfnxPa"},"source":["# Hyperparameters\n","env_name = 'CartPole-v1'\n","gamma = 0.99  # discount factor\n","seed = 543  # random seed\n","log_interval = 25  # controls how often we log progress, in episodes\n","num_steps = 5e4  # number of steps to train on\n","batch_size = 256  # batch size for optimization\n","lr = 1e-4  # learning rate\n","eps_start = 1.0  # initial value for epsilon (in epsilon-greedy)\n","eps_end = 0.1  # final value for epsilon (in epsilon-greedy)\n","eps_decay = num_steps  # length of epsilon decay, in env steps\n","target_update = 1000  # how often to update target net, in env steps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLb1oPuGoJpJ"},"source":["# Create environment\n","env = gym.make(env_name)\n","\n","# Fix random seed (for reproducibility)\n","env.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMHyB9Xyfaqm"},"source":["# Get number of actions from gym action space\n","n_inputs = env.observation_space.shape[0]\n","n_actions = env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tN_AtyQ1wsH1"},"source":["**Exercise #6.** Complete the call to `memory_push`."]},{"cell_type":"code","metadata":{"id":"9AGdVUq4nrWg"},"source":["if train_model:\n","  policy_net = DQN(n_inputs, n_actions).to(device)\n","  target_net = DQN(n_inputs, n_actions).to(device)\n","  target_net.load_state_dict(policy_net.state_dict())\n","  target_net.eval()\n","\n","  optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n","  memory = ReplayMemory(10000)\n","\n","  print(\"Target reward: {}\".format(env.spec.reward_threshold))\n","  step_count = 0\n","  ep_rew_history = []\n","  i_episode, ep_reward = 0, -float('inf')\n","  while step_count < num_steps:\n","      # Initialize the environment and state\n","      state, done = env.reset(), False\n","      state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n","      while not done:\n","          # Select an action\n","          eps_greedy_threshold = compute_eps_threshold(\n","              step_count, eps_start, eps_end, eps_decay)\n","          action = select_action(\n","              policy_net, state, eps_greedy_threshold, n_actions)\n","\n","          # Perform action in env\n","          next_state, reward, done, _ = env.step(action.item())\n","\n","          # Bookkeeping\n","          if done:\n","            # train() treats states as terminal when next_state is None\n","            next_state = None\n","          else:\n","            next_state = torch.from_numpy(\n","                next_state).float().unsqueeze(0).to(device)\n","          reward = torch.tensor([reward], device=device)\n","          step_count += 1\n","\n","          # Store the transition in memory\n","          memory.push(TODO)\n","\n","          # Move to the next state\n","          state = next_state\n","\n","          # Perform one step of the optimization (on the policy network)\n","          train(policy_net, target_net, optimizer, memory, batch_size, gamma)\n","          \n","          # Update the target network, copying all weights and biases in DQN\n","          if step_count % target_update == 0:\n","              target_net.load_state_dict(policy_net.state_dict())\n","\n","      i_episode += 1\n","\n","      # Evaluate greedy policy\n","      if i_episode % log_interval == 0 or step_count >= num_steps:\n","          ep_reward = test(env, policy_net)\n","          ep_rew_history.append((i_episode, ep_reward))\n","          print('Episode {}\\tSteps: {:.2f}k'\n","                '\\tEval reward: {:.2f}'.format(\n","                i_episode, step_count/1000., ep_reward))\n","\n","  print(\"Finished training! Eval reward: {:.2f}\".format(ep_reward))\n","  if not os.path.exists('checkpoints'):\n","    os.makedirs('checkpoints')\n","  torch.save(policy_net.state_dict(), 'checkpoints/dqn-{}.pt'.format(env_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9e8MsfLfUZcc"},"source":["if train_model:\n","  plt.plot([x[0] for x in ep_rew_history], [x[1] for x in ep_rew_history])\n","  plt.xlabel('Episode')\n","  plt.ylabel('Reward')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_cESm8xBnFj-"},"source":["## Visualize trained policy"]},{"cell_type":"code","metadata":{"id":"F97G8ZBUyPAN"},"source":["if load_model:\n","  policy_net = DQN(n_inputs, n_actions).to(device)\n","  policy_net.load_state_dict(\n","      torch.load('checkpoints/dqn-{}.pt'.format(env_name)))\n","test_env = wrap_env(gym.make(env_name))\n","ep_reward = test(test_env, policy_net, render=True)\n","print(\"Cumulative reward: {}\".format(ep_reward))\n","\n","test_env.close()\n","show_video()"],"execution_count":null,"outputs":[]}]}