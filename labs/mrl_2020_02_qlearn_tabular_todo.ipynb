{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mrl_2020_02_qlearn_tabular_todo.ipynb","provenance":[{"file_id":"1c_2lscX2bSUypGz6f62UHkOh6VN_EtUz","timestamp":1583853554133}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0KHRKexphbzM","colab_type":"text"},"source":["# Q-Learning in Python\n","\n","This notebook implements Q-Learning for tabular environments.\n","\n","**Notebook created by [Víctor Campos](https://imatge.upc.edu/web/people/victor-campos) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro)for the [Postgraduate course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).**\n","\n","![Víctor Campos](https://imatge.upc.edu/web/sites/default/files/styles/medium/public/users/vcampos//photo.jpg?itok=eCtqXNX9)\n","\n","![Xavier Giro-i-Nieto](https://telecombcn-dl.github.io/2019-dlcv/img/instructors/XavierGiro-160x160.jpg)"]},{"cell_type":"markdown","metadata":{"id":"wJH6uE5mh87e","colab_type":"text"},"source":["## Import dependencies\n","\n","We will use OpenAI Gym to simulate the environment and numpy to perform computations. "]},{"cell_type":"code","metadata":{"id":"ohP8NHPDiehm","colab_type":"code","colab":{}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook as tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdPIZ6puiO6X","colab_type":"text"},"source":["## Visualize the environment\n","\n","We will train an agent in the [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) environment. Read the documentation and make sure that you understand the structure of the problem before moving forward."]},{"cell_type":"markdown","metadata":{"id":"K-RoYGbKm1Bv","colab_type":"text"},"source":["**Exercise #1.** Visualize a rollout of a random agent in the environment. Use the [documentation](http://gym.openai.com/docs/) for OpenAI Gym as a reference."]},{"cell_type":"code","metadata":{"id":"kelQoY8XiCni","colab_type":"code","colab":{}},"source":["# Create an instance of the environment\n","env = gym.make(\"FrozenLake-v0\")\n","\n","# TODO: reset the environment\n","env.TODO\n","# Allow a maximum of 10 interactions\n","for t in range(10):\n","  print(\"\\nTimestep {}\".format(t))\n","  env.render()\n","  # TODO: sample a random action\n","  a = env.TODO\n","  # TODO: simulate the action in the environment; \n","  #       make sure to capture the 'done' signal\n","  ob, r, done, _ = env.TODO\n","  # Exit if the episode terminated\n","  if done:\n","    print(\"\\nEpisode terminated early\")\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJmruf19m_sD","colab_type":"text"},"source":["## Tabular Q-Learning Agent\n","\n","We will now implement an agent that performs Q-Learning in the tabular setting. It will maintain a table of Q(s,a) values, with shape `num_states x num_actions`, that will be updated online with the stream of experience collected by interacting with the environment.\n","\n","Exploration is critical in RL. The agent needs to continuously try actions that might seem suboptimal given its current beliefs in order to avoid getting trapped in local optima. We will use $\\epsilon$-greedy exploration for this purpose: a strategy that will sample random actions with probability $\\epsilon$, or will act greedily otherwise. We will decay $\\epsilon$ through the course of training, starting with a high value that favors exploration and slowly transitioning towards a more greedy policy.\n","\n","**Exercise #2.** Implement `QLearningAgent.greedy_action()`, a function that returns $\\text{argmax}_aQ(s,a)$.\n","\n","**Exercise #3.** Implement `QLearningAgent.eps_greedy_action()`, a function that returns a random action with probability $\\epsilon$ or $\\text{argmax}_aQ(s,a)$ otherwise.\n","\n","**Exercise #4.** Implement `QLearningAgent.update_q_values()`, a function that receives a tuple $(s_t,a_t,r_t,s_{t+1},d)$ and performs a TD update to the table of Q values. Pay special attention to the computation of the TD target for the last step of an episode (when `done==True`)."]},{"cell_type":"code","metadata":{"id":"_as8zIhujK5F","colab_type":"code","colab":{}},"source":["class QLearningAgent:\n","  \"\"\"Tabular Q-Learning Agent with epsilon-greedy exploration.\"\"\"\n","  def __init__(self, env_id, step_size=0.5, gamma=0.99,\n","               init_eps=1.0, final_eps=0.05, eps_decay_steps=50000):\n","    # Use separate env instances for training and testing\n","    self.train_env = gym.make(env_id)\n","    self.test_env = gym.make(env_id)\n","\n","    # Step size (this plays a similar role to the learning rate in SGD)\n","    self.step_size = step_size\n","\n","    # Discount factor\n","    self.gamma = gamma\n","\n","    # Epsilon, for epsilon-greedy exploration\n","    self.eps = init_eps\n","    self.init_eps = init_eps\n","    self.final_eps = final_eps\n","    self.eps_decay_steps = eps_decay_steps\n","    self.eps_delta = (self.final_eps - self.init_eps) / self.eps_decay_steps\n","\n","    # Table of Q-values, initialized to zero\n","    self.q = np.zeros(\n","        (self.train_env.observation_space.n, self.train_env.action_space.n))\n","    \n","    # Keep track of the current state of the training env\n","    self.s = self.train_env.reset()\n","\n","  def update_eps(self):\n","    \"\"\"Update the value of epsilon, ensuring that self.eps>=self.final_eps.\"\"\"\n","    self.eps = max(self.eps + self.eps_delta, self.final_eps)\n","  \n","  def greedy_action(self, s):\n","    # TODO: Returns argmax_a Q(s,a)\n","    return np.argmax(TODO)\n","\n","  def eps_greedy_action(self, s):\n","    # TODO: Returns random action with prob self.eps, or greedy action otherwise.\n","    if np.random.random() < self.eps:\n","      return self.TODO\n","    else:\n","      return self.TODO\n","\n","  def update_q_values(self, s, a, r, next_s, done):\n","    # TODO: Given a transition (s, a, r, s', done), perform a TD update to Q(s,a).\n","    if done:\n","      target = TODO\n","    else:\n","      target = TODO\n","    self.q[s, a] += self.step_size * (target - self.q[s, a])\n","\n","  def perform_train_step(self):\n","    \"\"\"Performs one RL interaction and updates the Q-values.\"\"\"\n","    # Act epsilon-greedily\n","    a = self.eps_greedy_action(self.s)\n","    next_s, r, done, _ = self.train_env.step(a)\n","\n","    # Update table of Q values\n","    self.update_q_values(self.s, a, r, next_s, done)\n","\n","    # Reset the env if the episode terminated\n","    if done:\n","      self.s = self.train_env.reset()\n","    else:\n","      self.s = next_s\n","    \n","    # Update epsilon\n","    self.update_eps()\n","\n","  def test(self, render=False):\n","    \"\"\"Perform an evaluation rollout with the greedy policy.\n","    Returns the cumulative reward.\"\"\"\n","    s = self.test_env.reset()\n","    done = False\n","    cumulative_r = 0.\n","    while not done:\n","      if render:\n","        self.test_env.render()\n","      s, r, done, _ = self.test_env.step(self.greedy_action(s))\n","      cumulative_r += r\n","    return cumulative_r"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LThVtUyJyZFM","colab_type":"text"},"source":["## Training loop\n","\n","We are now ready to train the agent. We will track the performance of the agent by performing evaluation rollouts periodically. In order to account for the stochasticity of the environment, the mean over several evaluation episodes is reported.\n","\n","**Exercise #5.** Train the agent with different hyperparameter configurations. Which ones have a larger influence in the results? "]},{"cell_type":"code","metadata":{"id":"GV_OKjMiyTfW","colab_type":"code","colab":{}},"source":["NUM_TRAINING_STEPS = 100000\n","EVALUATION_FREQ = 100\n","NUM_EVALUATION_EPISODES = 20\n","\n","agent = QLearningAgent(\"FrozenLake-v0\", \n","                       step_size=0.1, \n","                       gamma=0.99,\n","                       init_eps=1.0, \n","                       final_eps=0.1, \n","                       eps_decay_steps=NUM_TRAINING_STEPS)\n","\n","iter_history, rew_history = [], []\n","for iter_idx in tqdm(range(NUM_TRAINING_STEPS)):\n","  agent.perform_train_step()\n","  if iter_idx % EVALUATION_FREQ == 0 or iter_idx == (NUM_TRAINING_STEPS - 1):\n","    rew = np.mean([agent.test() for _ in range(NUM_EVALUATION_EPISODES)])\n","    iter_history.append(iter_idx + 1)\n","    rew_history.append(rew)\n","\n","# Plot results\n","fig, ax = plt.subplots(1, 1, figsize=(9,4))\n","ax.plot(iter_history, rew_history, label=\"Agent's reward\")\n","ax.plot(iter_history, \n","        [agent.train_env.spec.reward_threshold] * len(iter_history),\n","        'r--', label=\"Maximum reward\")\n","ax.set_xlabel(\"Environment steps\")\n","ax.set_ylabel(\"Reward\")\n","_ = ax.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Ml5xg7UU50W","colab_type":"text"},"source":["## Visualizing the learned policy"]},{"cell_type":"code","metadata":{"id":"Jiu2cR3H0UQX","colab_type":"code","colab":{}},"source":["agent.test(render=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S79El-931bBm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}